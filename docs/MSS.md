# Multimodal Semantic Segmentation

**Multimodal semantic segmentation involves parsing scenes and performing pixel-level classification based on the use of various perceptual modalities, such as optical, depth, infrared, SAR images, etc.**

## Natural Data

**1. Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation, _ECCV 2020_**
- Paper: https://link.springer.com/chapter/10.1007/978-3-030-58621-8_33
- Code: https://github.com/charlesCXK/RGBD_Semantic_Segmentation_PyTorch
```
@inproceedings{sagate,
  title={Bi-directional cross-modality feature propagation with separation-and-aggregation gate for RGB-D semantic segmentation},
  author={Chen, Xiaokang and Lin, Kwan-Yee and Wang, Jingbo and Wu, Wayne and Qian, Chen and Li, Hongsheng and Zeng, Gang},
  booktitle=ECCV,
  pages={561--577},
  year={2020},
  organization={Springer}
}
```

**2. Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, _CVPR 2021_**
- Paper: https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf
- Code: https://github.com/autonomousvision/transfuser/tree/cvpr2021
```
@inproceedings{transfuser,
  title={Multi-modal fusion transformer for end-to-end autonomous driving},
  author={Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
  booktitle=CVPR,
  pages={7077--7087},
  year={2021}
}
```

**3. Multimodal Token Fusion for Vision Transformers, _CVPR 2022_**
- Paper: https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Multimodal_Token_Fusion_for_Vision_Transformers_CVPR_2022_paper.pdf
- Code: https://github.com/yikaiw/TokenFusion
```
@inproceedings{tokenfusion,
  title={Multimodal token fusion for vision transformers},
  author={Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wenbing and Sun, Fuchun and Wang, Yunhe},
  booktitle=CVPR,
  pages={12186--12195},
  year={2022}
}
```

**4. MultiMAE: Multi-modal Multi-task Masked Autoencoders, _ECCV 2022_**
- Paper: https://link.springer.com/chapter/10.1007/978-3-031-19836-6_20
- Code: https://github.com/EPFL-VILAB/MultiMAE
```
@inproceedings{multimae,
  title={Multimae: Multi-modal multi-task masked autoencoders},
  author={Bachmann, Roman and Mizrahi, David and Atanov, Andrei and Zamir, Amir},
  booktitle=ECCV,
  pages={348--367},
  year={2022},
  organization={Springer}
}
```

**5. PGDENet: Progressive Guided Fusion and Depth Enhancement Network for RGB-D Indoor Scene Parsing, _TMM 2022_**
- Paper: https://ieeexplore.ieee.org/document/9740493
- Code: https://github.com/EnquanYang2022/PGDENet
```
@article{pgdenet,
  title={PGDENet: Progressive guided fusion and depth enhancement network for RGB-D indoor scene parsing},
  author={Zhou, Wujie and Yang, Enquan and Lei, Jingsheng and Wan, Jian and Yu, Lu},
  journal=TMM,
  volume={25},
  pages={3483--3494},
  year={2022},
  publisher={IEEE}
}
```

**6. CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers, _TITS 2023_**
- Paper: https://ieeexplore.ieee.org/document/10231003
- Code: https://github.com/huaaaliu/RGBX_Semantic_Segmentation
```
@article{cmx,
  title={CMX: Cross-modal fusion for RGB-X semantic segmentation with transformers},
  author={Zhang, Jiaming and Liu, Huayao and Yang, Kailun and Hu, Xinxin and Liu, Ruiping and Stiefelhagen, Rainer},
  journal=TITS,
  year={2023},
  publisher={IEEE}
}
```

**7. Delivering Arbitrary-Modal Semantic Segmentation, _CVPR 2023_**
- Paper: https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Delivering_Arbitrary-Modal_Semantic_Segmentation_CVPR_2023_paper.pdf
- Code: https://github.com/jamycheung/DELIVER
```
@inproceedings{cmnext,
  title={Delivering arbitrary-modal semantic segmentation},
  author={Zhang, Jiaming and Liu, Ruiping and Shi, Hao and Yang, Kailun and Rei{\ss}, Simon and Peng, Kunyu and Fu, Haodong and Wang, Kaiwei and Stiefelhagen, Rainer},
  booktitle=CVPR,
  pages={1136--1147},
  year={2023}
}
```

**8. DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation, _ICLR 2024_**
- Paper: https://arxiv.org/abs/2309.09668
- Code: https://github.com/VCIP-RGBD/DFormer
```
@inproceedings{dformer,
  title={Dformer: Rethinking rgbd representation learning for semantic segmentation},
  author={Yin, Bowen and Zhang, Xuying and Li, Zhongyu and Liu, Li and Cheng, Ming-Ming and Hou, Qibin},
  booktitle=ICLR,
  year={2024}
}
```

**9. Global feature-base multimodal semantic segmentation, _PR 2024_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0031320324000918
- Code: https://github.com/Sci-Epiphany/GFBNext
```
@article{gfbn,
  title={Global feature-based multimodal semantic segmentation},
  author={Gao, Suining and Yang, Xiubin and Jiang, Li and Fu, Zongqiang and Du, Jiamin},
  journal=PR,
  volume={151},
  pages={110340},
  year={2024},
  publisher={Elsevier}
}
```

**10. GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer, _ICML 2024_**
- Paper: https://arxiv.org/abs/2406.01210
- Code: https://github.com/JiaDingCN/GeminiFusion
```
@inproceedings{geminifusion,
  title={GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer},
  author={Jia, Ding and Guo, Jianyuan and Han, Kai and Wu, Han and Zhang, Chao and Xu, Chang and Chen, Xinghao},
  booktitle=ICML,
  year={2024}
}
```


## Remote Sensing Data