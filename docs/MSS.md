# Multimodal Semantic Segmentation

**Multimodal semantic segmentation involves parsing scenes and performing pixel-level classification based on the use of various perceptual modalities, such as optical, depth, infrared, SAR images, etc.**

## Natural Data

**1. Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation, _ECCV 2020_**
- Paper: https://link.springer.com/chapter/10.1007/978-3-030-58621-8_33
- Code: https://github.com/charlesCXK/RGBD_Semantic_Segmentation_PyTorch
```
@inproceedings{sagate,
  title={Bi-directional cross-modality feature propagation with separation-and-aggregation gate for RGB-D semantic segmentation},
  author={Chen, Xiaokang and Lin, Kwan-Yee and Wang, Jingbo and Wu, Wayne and Qian, Chen and Li, Hongsheng and Zeng, Gang},
  booktitle=ECCV,
  pages={561--577},
  year={2020},
  organization={Springer}
}
```

**2. Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, _CVPR 2021_**
- Paper: https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf
- Code: https://github.com/autonomousvision/transfuser/tree/cvpr2021
```
@inproceedings{transfuser,
  title={Multi-modal fusion transformer for end-to-end autonomous driving},
  author={Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
  booktitle=CVPR,
  pages={7077--7087},
  year={2021}
}
```

**3. Multimodal Token Fusion for Vision Transformers, _CVPR 2022_**
- Paper: https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Multimodal_Token_Fusion_for_Vision_Transformers_CVPR_2022_paper.pdf
- Code: https://github.com/yikaiw/TokenFusion
```
@inproceedings{tokenfusion,
  title={Multimodal token fusion for vision transformers},
  author={Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wenbing and Sun, Fuchun and Wang, Yunhe},
  booktitle=CVPR,
  pages={12186--12195},
  year={2022}
}
```

**4. MultiMAE: Multi-modal Multi-task Masked Autoencoders, _ECCV 2022_**
- Paper: https://link.springer.com/chapter/10.1007/978-3-031-19836-6_20
- Code: https://github.com/EPFL-VILAB/MultiMAE
```
@inproceedings{multimae,
  title={Multimae: Multi-modal multi-task masked autoencoders},
  author={Bachmann, Roman and Mizrahi, David and Atanov, Andrei and Zamir, Amir},
  booktitle=ECCV,
  pages={348--367},
  year={2022},
  organization={Springer}
}
```

**5. PGDENet: Progressive Guided Fusion and Depth Enhancement Network for RGB-D Indoor Scene Parsing, _TMM 2022_**
- Paper: https://ieeexplore.ieee.org/document/9740493
- Code: https://github.com/EnquanYang2022/PGDENet
```
@article{pgdenet,
  title={PGDENet: Progressive guided fusion and depth enhancement network for RGB-D indoor scene parsing},
  author={Zhou, Wujie and Yang, Enquan and Lei, Jingsheng and Wan, Jian and Yu, Lu},
  journal=TMM,
  volume={25},
  pages={3483--3494},
  year={2022},
  publisher={IEEE}
}
```

**6. CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers, _TITS 2023_**
- Paper: https://ieeexplore.ieee.org/document/10231003
- Code: https://github.com/huaaaliu/RGBX_Semantic_Segmentation
```
@article{cmx,
  title={CMX: Cross-modal fusion for RGB-X semantic segmentation with transformers},
  author={Zhang, Jiaming and Liu, Huayao and Yang, Kailun and Hu, Xinxin and Liu, Ruiping and Stiefelhagen, Rainer},
  journal=TITS,
  year={2023},
  publisher={IEEE}
}
```

**7. Delivering Arbitrary-Modal Semantic Segmentation, _CVPR 2023_**
- Paper: https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Delivering_Arbitrary-Modal_Semantic_Segmentation_CVPR_2023_paper.pdf
- Code: https://github.com/jamycheung/DELIVER
```
@inproceedings{cmnext,
  title={Delivering arbitrary-modal semantic segmentation},
  author={Zhang, Jiaming and Liu, Ruiping and Shi, Hao and Yang, Kailun and Rei{\ss}, Simon and Peng, Kunyu and Fu, Haodong and Wang, Kaiwei and Stiefelhagen, Rainer},
  booktitle=CVPR,
  pages={1136--1147},
  year={2023}
}
```

**8. DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation, _ICLR 2024_**
- Paper: https://arxiv.org/abs/2309.09668
- Code: https://github.com/VCIP-RGBD/DFormer
```
@inproceedings{dformer,
  title={Dformer: Rethinking rgbd representation learning for semantic segmentation},
  author={Yin, Bowen and Zhang, Xuying and Li, Zhongyu and Liu, Li and Cheng, Ming-Ming and Hou, Qibin},
  booktitle=ICLR,
  year={2024}
}
```

**9. Global feature-base multimodal semantic segmentation, _PR 2024_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0031320324000918
- Code: https://github.com/Sci-Epiphany/GFBNext
```
@article{gfbn,
  title={Global feature-based multimodal semantic segmentation},
  author={Gao, Suining and Yang, Xiubin and Jiang, Li and Fu, Zongqiang and Du, Jiamin},
  journal=PR,
  volume={151},
  pages={110340},
  year={2024},
  publisher={Elsevier}
}
```

**10. GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer, _ICML 2024_**
- Paper: https://arxiv.org/abs/2406.01210
- Code: https://github.com/JiaDingCN/GeminiFusion
```
@inproceedings{geminifusion,
  title={GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer},
  author={Jia, Ding and Guo, Jianyuan and Han, Kai and Wu, Han and Zhang, Chao and Xu, Chang and Chen, Xinghao},
  booktitle=ICML,
  year={2024}
}
```


## Remote Sensing Data

**1. MCANet: A joint semantic segmentation framework of optical and SAR images for land use classification, _JAG 2022_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0303243421003457
- Code: https://github.com/AmberHen/WHU-OPT-SAR-dataset
```
@article{mcanet,
  title={MCANet: A joint semantic segmentation framework of optical and SAR images for land use classification},
  author={Li, Xue and Zhang, Guo and Cui, Hao and Hou, Shasha and Wang, Shunyao and Li, Xin and Chen, Yujia and Li, Zhijiang and Zhang, Li},
  journal=JAG,
  volume={106},
  pages={102638},
  year={2022},
  publisher={Elsevier}
}
```

**2. CMGFNet: A deep cross-modal gated fusion network for building extraction from very high-resolution remote sensing images, _ISPRS 2022_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0924271621003294
- Code: https://github.com/hamidreza2015/CMGFNet-Building_Extraction
```
@article{cmgfnet,
  title={CMGFNet: A deep cross-modal gated fusion network for building extraction from very high-resolution remote sensing images},
  author={Hosseinpour, Hamidreza and Samadzadegan, Farhad and Javan, Farzaneh Dadrass},
  journal=ISPRS,
  volume={184},
  pages={96--115},
  year={2022},
  publisher={Elsevier}
}
```

**3. Category-Wise Fusion and Enhancement Learning for Multimodal Remote Sensing Image Semantic Segmentation, _TGRS 2022_**
- Paper: https://ieeexplore.ieee.org/document/9968034
- Code: https://github.com/aihuazheng/CAFE
```
@article{cafe,
  title={Category-wise fusion and enhancement learning for multimodal remote sensing image semantic segmentation},
  author={Zheng, Aihua and He, Jinbo and Wang, Ming and Li, Chenglong and Luo, Bin},
  journal=TGRS,
  volume={60},
  pages={1--12},
  year={2022},
  publisher={IEEE}
}
```

**4. A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data, _JSTARS 2022_**
- Paper: https://ieeexplore.ieee.org/document/9749821
- Code: https://github.com/sstary/SSRS
```
@article{cmfnet,
  title={A crossmodal multiscale fusion network for semantic segmentation of remote sensing data},
  author={Ma, Xianping and Zhang, Xiaokang and Pun, Man-On},
  journal=JSTARS,
  volume={15},
  pages={3463--3474},
  year={2022},
  publisher={IEEE}
}
```

**5. Progressive fusion learning: A multimodal joint segmentation framework for building extraction from optical and SAR images, _ISPRS 2023_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0924271622003082
- Code: https://github.com/AmberHen/Progressive-fusion-learning
```
@article{pfl,
  title={Progressive fusion learning: A multimodal joint segmentation framework for building extraction from optical and SAR images},
  author={Li, Xue and Zhang, Guo and Cui, Hao and Hou, Shasha and Chen, Yujia and Li, Zhijiang and Li, Haifeng and Wang, Huabin},
  journal=ISPRS,
  volume={195},
  pages={178--191},
  year={2023},
  publisher={Elsevier}
}
```

**6. Aligning semantic distribution in fusing optical and SAR images for land use classification, _ISPRS 2023_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0924271623000977
- Code: https://github.com/Margin1996/ASD
```
@article{asd,
  title={Aligning semantic distribution in fusing optical and SAR images for land use classification},
  author={Li, Wangbin and Sun, Kaimin and Li, Wenzhuo and Wei, Jinjiang and Miao, Shunxia and Gao, Song and Zhou, Qinhui},
  journal=ISPRS,
  volume={199},
  pages={272--288},
  year={2023},
  publisher={Elsevier}
}
```

**7. A Multilevel Multimodal Fusion Transformer for Remote Sensing Semantic Segmentation, _TGRS 2024_**
- Paper: https://ieeexplore.ieee.org/document/10458980
- Code: https://github.com/sstary/SSRS
```
@article{ftransunet,
  title={A multilevel multimodal fusion transformer for remote sensing semantic segmentation},
  author={Ma, Xianping and Zhang, Xiaokang and Pun, Man-On and Liu, Ming},
  journal=TGRS,
  year={2024},
  publisher={IEEE}
}
```

**8. Assisted learning for land use classification: The important role of semantic correlation between heterogeneous images, _ISPRS 2024_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0924271623003441
- Code: https://github.com/Margin1996/Assisted_learning
```
@article{assisted_learning,
  title={Assisted learning for land use classification: The important role of semantic correlation between heterogeneous images},
  author={Li, Wangbin and Sun, Kaimin and Li, Wenzhuo and Huang, Xiao and Wei, Jinjiang and Chen, Yepei and Cui, Wei and Chen, Xueyu and Lv, Xianwei},
  journal=ISPRS,
  volume={208},
  pages={158--175},
  year={2024},
  publisher={Elsevier}
}
```

**9. SoftFormer: SAR-optical fusion transformer for urban land use and land cover classification, _ISPRS 2024_**
- Paper: https://www.sciencedirect.com/science/article/pii/S0924271624003502
- Code: https://github.com/rl1024/SoftFormer
```
@article{softformer,
  title={SoftFormer: SAR-optical fusion transformer for urban land use and land cover classification},
  author={Liu, Rui and Ling, Jing and Zhang, Hongsheng},
  journal=ISPRS,
  volume={218},
  pages={277--293},
  year={2024},
  publisher={Elsevier}
}
```

**10. MultiSenseSeg: A Cost-Effective Unified Multimodal Semantic Segmentation Model for Remote Sensing, _TGRS 2024_**
- Paper: https://ieeexplore.ieee.org/document/10504922
- Code: https://github.com/W-qp/MultiSenseSeg
```
@article{multisenseseg,
  title={MultiSenseSeg: A cost-effective unified multimodal semantic segmentation model for remote sensing},
  author={Wang, Qingpeng and Chen, Wei and Huang, Zhou and Tang, Hongzhao and Yang, Lan},
  journal=TGRS,
  year={2024},
  publisher={IEEE}
}
```