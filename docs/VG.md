# Visual Grounding

**Visual Grounding, also called Referring Expression Comprehension, is a fundamental vision-language task that aims to localize out an object referred to by a natural language expression from an image.**

## Natural Data

**1. TransVG: End-to-End Visual Grounding with Transformers, _ICCV 2021_**
- Paper: https://openaccess.thecvf.com/content/ICCV2021/papers/Deng_TransVG_End-to-End_Visual_Grounding_With_Transformers_ICCV_2021_paper.pdf
- Code: https://github.com/djiajunustc/TransVG
```
@inproceedings{transvg,
  title={Transvg: End-to-end visual grounding with transformers},
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle=ICCV,
  pages={1769--1779},
  year={2021}
}
```

**2. Referring Transformer: A One-step Approach to Multi-task Visual Grounding, _NIPS 2021_**
- Paper: https://proceedings.neurips.cc/paper_files/paper/2021/file/a376802c0811f1b9088828288eb0d3f0-Paper.pdf
- Code: https://github.com/ubc-vision/RefTR
```
@article{reftr,
  title={Referring transformer: A one-step approach to multi-task visual grounding},
  author={Li, Muchen and Sigal, Leonid},
  journal=NIPS,
  volume={34},
  pages={19652--19664},
  year={2021}
}
```

**3. Improving Visual Grounding With Visual-Linguistic Verification and Iterative Reasoning, _CVPR 2022_**
- Paper: https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Improving_Visual_Grounding_With_Visual-Linguistic_Verification_and_Iterative_Reasoning_CVPR_2022_paper.pdf
- Code: https://github.com/yangli18/VLTVG
```
@inproceedings{vltvg,
  title={Improving visual grounding with visual-linguistic verification and iterative reasoning},
  author={Yang, Li and Xu, Yan and Yuan, Chunfeng and Liu, Wei and Li, Bing and Hu, Weiming},
  booktitle=CVPR,
  pages={9499--9508},
  year={2022}
}
```

**4. Shifting More Attention to Visual Backbone: Query-Modulated Refinement Networks for End-to-End Visual Grounding, _CVPR 2022_**
- Paper: https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Shifting_More_Attention_to_Visual_Backbone_Query-Modulated_Refinement_Networks_for_CVPR_2022_paper.pdf
- Code: https://github.com/LukeForeverYoung/QRNet
```
@inproceedings{qrnet,
  title={Shifting more attention to visual backbone: Query-modulated refinement networks for end-to-end visual grounding},
  author={Ye, Jiabo and Tian, Junfeng and Yan, Ming and Yang, Xiaoshan and Wang, Xuwu and Zhang, Ji and He, Liang and Lin, Xin},
  booktitle=CVPR,
  pages={15502--15512},
  year={2022}
}
```

**5. Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding, _CVPR 2022_**
- Paper: https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf
- Code: https://github.com/LeapLabTHU/Pseudo-Q
```
@inproceedings{pseudoq,
  title={Pseudo-q: Generating pseudo language queries for visual grounding},
  author={Jiang, Haojun and Lin, Yuanze and Han, Dongchen and Song, Shiji and Huang, Gao},
  booktitle=CVPR,
  pages={15513--15523},
  year={2022}
}
```

**6. PolyFormer: Referring Image Segmentation as Sequential Polygon Generation, _CVPR 2023_**
- Paper: https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf
- Code: https://github.com/amazon-science/polygon-transformer
```
@inproceedings{polyformer,
  title={Polyformer: Referring image segmentation as sequential polygon generation},
  author={Liu, Jiang and Ding, Hui and Cai, Zhaowei and Zhang, Yuting and Satzoda, Ravi Kumar and Mahadevan, Vijay and Manmatha, R},
  booktitle=CVPR,
  pages={18653--18663},
  year={2023}
}
```

**7. TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer, _TPAMI 2023_**
- Paper: https://ieeexplore.ieee.org/document/10187690
- Code: None
```
@article{transvg++,
  title={Transvg++: End-to-end visual grounding with language conditioned vision transformer},
  author={Deng, Jiajun and Yang, Zhengyuan and Liu, Daqing and Chen, Tianlang and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang and Ouyang, Wanli},
  journal=TPAMI,
  year={2023},
  publisher={IEEE}
}
```

**8. Context Disentangling and Prototype Inheriting for Robust Visual Grounding, _TPAMI 2023_**
- Paper: https://ieeexplore.ieee.org/document/10342826
- Code: https://github.com/WayneTomas/TransCP
```
@article{transcp,
  title={Context disentangling and prototype inheriting for robust visual grounding},
  author={Tang, Wei and Li, Liang and Liu, Xuejing and Jin, Lu and Tang, Jinhui and Li, Zechao},
  journal=TPAMI,
  year={2023},
  publisher={IEEE}
}
```

**9. CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding, _TMM 2023_**
- Paper: https://ieeexplore.ieee.org/document/10269126
- Code: https://github.com/linhuixiao/CLIP-VG
```
@article{clipvg,
  title={CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding},
  author={Xiao, Linhui and Yang, Xiaoshan and Peng, Fang and Yan, Ming and Wang, Yaowei and Xu, Changsheng},
  journal=TMM,
  year={2023},
  publisher={IEEE}
}
```

**10. HiVG: Hierarchical Multimodal Fine-grained Modulation for Visual Grounding, _ACMMM 2024_**
- Paper: https://arxiv.org/pdf/2404.13400
- Code: https://github.com/linhuixiao/HiVG
```
@inproceedings{hivg,
  title={HiVG: Hierarchical Multimodal Fine-grained Modulation for Visual Grounding},
  author={Xiao, Linhui and Yang, Xiaoshan and Peng, Fang and Wang, Yaowei and Xu, Changsheng},
  booktitle=ACMMM,
  year={2024},
}
```

**11. SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion, _NIPS 2024_**
- Paper: https://arxiv.org/pdf/2409.17531v1
- Code: https://github.com/dmmm1997/simvg
```
@article{simvg,
  title={SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion},
  author={Dai, Ming and Yang, Lingfeng and Xu, Yihao and Feng, Zhenhua and Yang, Wankou},
  journal=NIPS,
  year={2024}
}
```


## Remote Sensing Data

**1. Visual Grounding in Remote Sensing Images, _ACMMM 2022_**
- Paper: https://dl.acm.org/doi/abs/10.1145/3503161.3548316
- Code: https://sunyuxi.github.io/publication/GeoVG
```
@inproceedings{geovg,
  title={Visual grounding in remote sensing images},
  author={Sun, Yuxi and Feng, Shanshan and Li, Xutao and Ye, Yunming and Kang, Jian and Huang, Xu},
  booktitle=ACMMM,
  pages={404--412},
  year={2022}
}
```

**2. RSVG: Exploring Data and Models for Visual Grounding on Remote Sensing Data, _TGRS 2023_**
- Paper: https://ieeexplore.ieee.org/document/10056343
- Code: https://github.com/ZhanYang-nwpu/RSVG-pytorch
```
@article{mgvlf,
  title={Rsvg: Exploring data and models for visual grounding on remote sensing data},
  author={Zhan, Yang and Xiong, Zhitong and Yuan, Yuan},
  journal=TGRS,
  volume={61},
  pages={1--13},
  year={2023},
  publisher={IEEE}
}
```

**3. Multistage Synergistic Aggregation Network for Remote Sensing Visual Grounding, _GRSL 2024_**
- Paper: https://ieeexplore.ieee.org/document/10417055
- Code: https://github.com/waynamigo/MSAM
```
@article{msam,
  title={Multi-Stage Synergistic Aggregation Network for Remote Sensing Visual Grounding},
  author={Wang, Fuyan and Wu, Chunlei and Wu, Jie and Wang, Leiquan and Li, Canwei},
  journal=GRSL,
  year={2024},
  publisher={IEEE}
}
```

**4. Language Query-Based Transformer With Multiscale Cross-Modal Alignment for Visual Grounding on Remote Sensing Images, _TGRS 2024_**
- Paper: https://ieeexplore.ieee.org/document/10542207
- Code: https://github.com/LANMNG/LQVG
```
@article{lqvg,
  title={Language query based transformer with multi-scale cross-modal alignment for visual grounding on remote sensing images},
  author={Lan, Meng and Rong, Fu and Jiao, Hongzan and Gao, Zhi and Zhang, Lefei},
  journal=TGRS,
  year={2024},
  publisher={IEEE}
}
```

**5. Language-Guided Progressive Attention for Visual Grounding in Remote Sensing Images, _TGRS 2024_**
- Paper: https://ieeexplore.ieee.org/document/10584552
- Code: https://github.com/like413/OPT-RSVG
```
@article{lpva,
  title={Language-Guided Progressive Attention for Visual Grounding in Remote Sensing Images},
  author={Li, Ke and Wang, Di and Xu, Haojie and Zhong, Haodi and Wang, Cong},
  journal=TGRS,
  year={2024},
  publisher={IEEE}
}
```